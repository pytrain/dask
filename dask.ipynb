{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parallel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkdK_gN1sUvc",
        "colab_type": "text"
      },
      "source": [
        "<center><img src=\"http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg\"></center>\n",
        "\n",
        "<center>\n",
        "<h1><font size=\"+3\">GSFC Python Bootcamp</font></h1>\n",
        "</center>\n",
        "\n",
        "---\n",
        "\n",
        "<center><h3>Parallel Programming</h3></center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xImHxuCylPpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8sN_3lHl0C4",
        "colab_type": "text"
      },
      "source": [
        "There are other ways to do multiprocessing within Python, but here, we wanted to give you an entry point to this parallel programming methodology. Below are other links to the other packages if you so choose to learn them:\n",
        "\n",
        "* [multithreading](https://docs.python.org/3.4/library/threading.html) - to use multiple threads to perform computations\n",
        "* [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) - to use multiple processors to perform computations\n",
        "* [asyncio](https://docs.python.org/3/library/asyncio.html) - to schedule asynchronous (concurrent) tasks such as input and output or calculations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgUyuVxPmt8m",
        "colab_type": "text"
      },
      "source": [
        "__Warning:__ There is a topic that you will eventually encounter when programming in parallel. This is the [GIL](https://wiki.python.org/moin/GlobalInterpreterLock) (Global Interpreter Lock)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCCBcXIflTnm",
        "colab_type": "text"
      },
      "source": [
        "# Outline\n",
        "\n",
        "1. What is Dask?\n",
        "2. A simple example.\n",
        "3. Using Dask with Bokeh server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsBG6rnElbxD",
        "colab_type": "text"
      },
      "source": [
        "## 1. What is Dask?\n",
        "\n",
        "---\n",
        "\n",
        "[Dask](https://dask.org/) is smply a Python library/package to add parallel computing capabilities. It primarily is constructed to two parts (I would consider 3, but the documentation says 2):\n",
        "\n",
        "* __Dynamic task scheduler:__ this is the worker that once Dask has a task graph created, this portion handles the execution on parallel hardware whether it be a single machine or a cluster\n",
        "* __Big data collections:__ think the usefulness of NumPy and Pandas APIs but used within Dask\n",
        "* _Task graphs:_ Dask's underlying step that connects the data collections and task schedulers are these graphs that Dask creates before using the functionality of Pandas or scheduling a task through the scheduler."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK6fir3g6Cls",
        "colab_type": "text"
      },
      "source": [
        "__Note:__ Dask is NOT [Dash](https://plot.ly/products/dash/) which works with the Bokeh plotting package. Dash is the package to create dashboard-like applications using Plotly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0_a-JvNnN4K",
        "colab_type": "text"
      },
      "source": [
        "## Why Dask?\n",
        "\n",
        "---\n",
        "\n",
        "Python, in general, is a great tool for data scientists. However, packages used by these scientists such as NumPy, Pandas, and so forth were not designed for computation beyond a single machine. Dask takes the usefulness of these packages and extends them to be used across multiple machines for larger applications, computations and analysis while still being able to scale back down to be used on a single machine. Dask also maintains it's familiarity of these packages for those that have used them before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7slU4RNnR1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Arrays implement the Numpy API\n",
        "import dask.array as da\n",
        "x = da.random.random(size=(10000, 10000),\n",
        "                     chunks=(1000, 1000))\n",
        "x + x.T - x.mean(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtUiUH4MnUsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataframes implement the Pandas API\n",
        "import dask\n",
        "df = dask.datasets.timeseries()\n",
        "len(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80jzviSGnWsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiyG5iNLnYMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = df.groupby(df.id).x.sum()\n",
        "print(df2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f436DiTfniXv",
        "colab_type": "text"
      },
      "source": [
        "At this point, dask has not computed the actual sum. We have to tell Dask to compute this explicitly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzS6YKkwncfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.compute()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpStWsBHnriK",
        "colab_type": "text"
      },
      "source": [
        "## 2. A simple example\n",
        "\n",
        "---\n",
        "\n",
        "Here, we will go beyond just calculating in series and try to use Dask's distributed module to do calculations in parallel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lz6iv-8nq5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import sleep\n",
        "import random\n",
        "\n",
        "def inc(x):\n",
        "    sleep(0.2)\n",
        "    return x + 1\n",
        "\n",
        "def double(x):\n",
        "    sleep(0.2)\n",
        "    return 2 * x\n",
        "\n",
        "def add(x,y):\n",
        "    sleep(0.2)\n",
        "    return x + y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EIQZ8jdnyw9",
        "colab_type": "text"
      },
      "source": [
        "Serial/Sequential calculations to obtain a mathematical total. (Benchmark ~5 seconds on a Mac laptop.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCv5_DGan1Uq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%timeit -r 1\n",
        "\n",
        "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "\n",
        "out = []\n",
        "for x in data:\n",
        "    y = inc(x)\n",
        "    z = double(y)\n",
        "    out.append(z)\n",
        "    \n",
        "total = 0\n",
        "for z in out:\n",
        "    total = add(total, z)\n",
        "\n",
        "total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmskfw-Xn4jZ",
        "colab_type": "text"
      },
      "source": [
        "### Update\n",
        "\n",
        "> We tell dask in the code below that each of these methods are going to be parallelized by wrapping them as dask delayed functions. The functions have not been executed in this block below, merely proxy objects that have a graph associated with it (see visualize below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-k7T2ytn8pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dask\n",
        "\n",
        "# delayed means to setup but not compute yet\n",
        "inc = dask.delayed(inc)\n",
        "double = dask.delayed(double)\n",
        "add = dask.delayed(add)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1M3DZa8n_wb",
        "colab_type": "text"
      },
      "source": [
        "### Update\n",
        "\n",
        "> The __visualize__ method below requires the _graphviz_ package. I have tried installing it via __conda install__, but what worked for me was __pip install__ on the command line in the appropriate Python environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ3tR5PdoKHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = inc(1)\n",
        "y = inc(2)\n",
        "z = add(x, y)\n",
        "dask.visualize(z, rankdir='LR')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjLDgnqKoNsj",
        "colab_type": "text"
      },
      "source": [
        "### Update\n",
        "\n",
        "> The next code block now uses our wrapped/delayed methods to create a delayed total. It has not physically calculated the total in which makes this execution very fast. The actual computation follows afterwards via the __dask.compute__ call."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKl19sWWoST1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "\n",
        "out = []\n",
        "for x in data:\n",
        "    y = inc(x)\n",
        "    z = double(y)\n",
        "    out.append(z)\n",
        "    \n",
        "total = 0\n",
        "for z in out:\n",
        "    total = add(total, z)\n",
        "    \n",
        "total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM2W333loWmu",
        "colab_type": "text"
      },
      "source": [
        "### Update\n",
        "\n",
        "> Now we have our new delayed methods that will be parallelized rather than sequential. To schedule or perform the processes/calculations we tell dask to compute the total. You will notice that it is faster (~2 seconds)than the prior sequential computation of roughly ~5 seconds on a Mac laptop.\n",
        "> \n",
        "> It also returns the correct calculated value as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4pXsBdkochO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%timeit -r 1\n",
        "dask.compute(total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkQctc5koeoj",
        "colab_type": "text"
      },
      "source": [
        "Notice that even though this was parallelized, there are still parts that are sequential in computation. This hints at better parallelization algorithms/schemes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVQ9bIk3orLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dask.visualize(total, rankdir='LR') # sequential dependence still evident by visualization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y20icjraotzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "\n",
        "out = []\n",
        "for x in data:\n",
        "    y = inc(x)\n",
        "    z = double(y)\n",
        "    out.append(z)\n",
        "    \n",
        "# tree reduction\n",
        "while len(out) > 1:\n",
        "    out = [add(out[i], out[i+1]) for i in range(0, len(out), 2)]\n",
        "\n",
        "total = out[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R05WEcfrowFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dask.visualize(total, rankdir='LR')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io5IS0D1oyYg",
        "colab_type": "text"
      },
      "source": [
        "### Update\n",
        "\n",
        "> While this new method of the tree reduction seems fast, remember that it has not actually performed the calculation in question. Let's do that now.\n",
        "> \n",
        "> I saw a reduction in time to ~1 second with this new and more appropriate algorithm for summations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSLg5mngo30l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%timeit -r 1\n",
        "\n",
        "dask.compute(total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFxK7hNUo7Pj",
        "colab_type": "text"
      },
      "source": [
        "We can instantly see which of these two algorithms will be better for parallelization. But what if we had a more complex computation..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCKkTzb1o9s0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dask.array as da\n",
        "\n",
        "# 15x15 array of ones chunked into 5x5 squares (uses NumPy mainly)\n",
        "x = da.ones((15, 15), chunks=(5,5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vdlWsbyo_1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dask.visualize(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOfrH5v-pCRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dask.visualize((x.dot(x.T) - x.mean(axis=0)).std())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzADI0CopGne",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "But this graph does not give us any indication whether our parallelization is efficient or not (as well as using resources appropriately). We need a way to visualize how it __performs__ on a system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJv9INhMpK7K",
        "colab_type": "text"
      },
      "source": [
        "## 3. Using Dask with Bokeh Server\n",
        "\n",
        "---\n",
        "\n",
        "Locally, here are the steps to carete the dask bokeh server:\n",
        "\n",
        "1. You need 2 terminal/command prompt sessions/windows that have the appropriate Python environment activated/sourced.\n",
        "2. Execute dask-scheduler in the first session/window. If you do not have this command line utilty, you need to install the _\"distributed\"_ package. [Link](http://distributed.readthedocs.io/en/latest/install.html)\n",
        "3. The scheduler will give addresses:\n",
        "  * scheduler: this is the address we use for the workers\n",
        "  * http: to view the local web server, copy this address into a browser\n",
        "  * bokeh: this is where our bokeh server/visualizer/dashboard will be\n",
        "4. Schedule a dask worker for the scheduler in the next terminal session/window:\n",
        "\n",
        "\n",
        "        dask-worker xxx.xxx.xxx.xxx:xxxx\n",
        "\n",
        "\n",
        "    By using the address given from the scheduler (minus the tcp part).\n",
        "5. Change the code below in the jupyter notebook to reflect the dask-scheduler address.\n",
        "6. Open a new browser tab/window and go to the bokeh server address from the __dask-scheduler__ information. If you used the one from the worker, you will not have all the utilities available.\n",
        "7. Click on the __status__ text to view the dashboard for processes running.\n",
        "8. Execute the code below from the notebook and view the output in the dask bokeh dashboard. You can repeatedly do this and modify the parallelization to get better performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF17w7uQqD0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from distributed import Client, LocalCluster\n",
        "from time import sleep\n",
        "import random\n",
        "import dask\n",
        "\n",
        "def inc(x):\n",
        "    sleep(random.random() / 10)\n",
        "    return x + 1\n",
        "\n",
        "def dec(x):\n",
        "    sleep(random.random() / 10)\n",
        "    return x - 1\n",
        "\n",
        "def add(x, y):\n",
        "    sleep(random.random() / 10)\n",
        "    return x + y\n",
        "\n",
        "\n",
        "# change this to the address and port from your dask bokeh server\n",
        "client = Client('172.28.0.2:8786')\n",
        "\n",
        "incs = client.map(inc, range(100))\n",
        "decs = client.map(dec, range(100))\n",
        "adds = client.map(add, incs, decs)\n",
        "total = client.submit(sum, adds)\n",
        "\n",
        "del incs, decs, adds\n",
        "total.result()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}